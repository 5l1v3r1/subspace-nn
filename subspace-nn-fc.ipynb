{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a 784-200-200-10 MNIST classifier in $\\omega$-space\n",
    "\n",
    "Sam Greydanus. June 2017. MIT License.\n",
    "\n",
    "$$\\arg\\min_{\\mathbf{\\omega}}  -\\frac{1}{n} \\sum_X (y\\ln \\hat y +(1-y)\\ln (1-\\hat y)) \\quad \\mathrm{where} \\quad \\hat y = f_{NN}(\\theta, X) \\quad \\mathrm{and} \\quad \\theta = P \\omega$$\n",
    "\n",
    "**Inituitively,** we want to express the parameters, $\\theta$, of a neural network as the product of a projector, $P$, and a vector, $\\omega$, of trainable parameters that occupy a smaller space (e.g. of dimension 10-1000). During training, we'll backprop into and optimize in $\\omega$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from utils import *\n",
    "\n",
    "reseed = lambda: np.random.seed(seed=123) ; ms = torch.manual_seed(123) # for reproducibility\n",
    "reseed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "This gets a little hacky because I'm trying to mimic `argparse`, which I use in `main.py` to set hyperparameters. Doing this is worthwhile because it allows me to keep the rest of the syntax identical between this notebook and the `main.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 50 epochs (11718 steps)\n"
     ]
    }
   ],
   "source": [
    "class DummyArgparse():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = DummyArgparse()\n",
    "\n",
    "args.fig_dir = 'figures/'      # directory to save figures\n",
    "args.hidden_dim = 200          # hidden dimension of neural network\n",
    "args.batch_size = 256          # batch size for training\n",
    "args.omega_dims = [None, 3, 10, 100, 300, 1000] # sizes for omega subspaces\n",
    "args.lr = 3e-3                 # learning rate\n",
    "args.test_every = 100          # record test accuracy after this number of steps\n",
    "args.epochs = 50               # number of epochs to train\n",
    "\n",
    "# book-keeping\n",
    "os.makedirs(args.fig_dir) if not os.path.exists(args.fig_dir) else None\n",
    "args.input_dim = 28**2\n",
    "args.target_dim = 10\n",
    "global_step = 0\n",
    "total_steps = int(60000*args.epochs/args.batch_size)\n",
    "print(\"using {} epochs ({} steps)\".format(args.epochs, total_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader\n",
    "This object is an interface to the PyTorch MNIST utility. It lets us access either the train or test sets, depending on the mode we request. Training samples from the MNIST dataset look like this:\n",
    "\n",
    "<img src=\"static/mnist.png\" alt=\"MNIST training samples\" style=\"width: 250px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make dataloader\n",
    "dataloader = Dataloader(args.batch_size, args.input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subspace NN\n",
    "I could also have written `from subspace_nn import SubspaceNN`, but this class is the meat of this project so I wanted to show it off.\n",
    "\n",
    "**Overview.** This is a modified two-layer 784-200-200-10 neural network for classifying MNIST digits. If the `omega_dim` parameter is not set to `None`, the class will initialize its trainable parameters in a subspace of dimension $\\omega$ and construct a projector, $P$, which maps from $\\omega$-space to $\\theta$-space:\n",
    "$$\\theta = P \\omega$$\n",
    "The class will initialize its trainable parameters in $\\theta$-space as usual if `omega_dim` is `None`.\n",
    "\n",
    "**Initialization.** The $\\omega$ subspace makes initialization tricky. First, we perform initialization in $\\theta$-space as usual, then use the initialized vector, $\\theta_i$ as the first column vector in the projector $P$. We obtain the other columns of $P$ using [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) to obtain an orthonormal basis. Finally, we set all entries of the $\\omega_i$ vector to $0$ except the first, which we set to $a = \\theta_i[0] / P[0,0]$. We chose $a$ in this way because each column vector of $P$ has been normalized and therefore the first column vector differs from $\\theta_i$ by a factor of $a$.\n",
    "\n",
    "**Feedforward.** The feedforward part looks the same regardless of whether we're training in $\\omega$-space or $\\theta$-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SubspaceNN(torch.nn.Module):\n",
    "    def __init__(self, batch_size, input_dim, h_dim, output_dim, omega_dim=None):\n",
    "        super(SubspaceNN, self).__init__()\n",
    "        # param_meta maps each param to (dim1, dim2, initial_stdev)\n",
    "        self.batch_size = batch_size\n",
    "        self.omega_dim = omega_dim\n",
    "        self.param_meta = {'W1': (input_dim, h_dim, 0.001), 'W2': (h_dim, h_dim, 0.001),\n",
    "                      'W3': (h_dim, output_dim, 0.01), \\\n",
    "                      'b1': (1, h_dim, 0.0), 'b2': (1, h_dim, 0.0), 'b3': (1, output_dim, 0.0) }\n",
    "                      \n",
    "        self.names = [k for k in self.param_meta.keys()]\n",
    "        self.counts = [self.param_meta[n][0]*self.param_meta[n][1] for n in self.names]\n",
    "        self.slices = np.cumsum([0] + self.counts)\n",
    "        self.theta_dim = int(self.slices[-1])\n",
    "        \n",
    "        self.init_param_space()\n",
    "        print('model summary:\\n\\tthis model\\'s omega space has {} parameters'.format(self.omega_dim))\n",
    "        print('\\tthis model\\'s theta space has {} parameters'.format(self.theta_dim))\n",
    "        \n",
    "    def init_param_space(self):\n",
    "        # initialize and concat\n",
    "        flat_params = [np.random.randn(self.counts[i],1)*self.param_meta[n][2] for i, n in enumerate(self.names)]\n",
    "        theta_init = np.concatenate(flat_params, axis=0)\n",
    "        if self.omega_dim is None:\n",
    "            self.flat_theta = nn.Parameter(torch.Tensor(theta_init), requires_grad=True)\n",
    "        else:\n",
    "            random_init = np.random.randn(self.theta_dim, self.omega_dim-1)\n",
    "\n",
    "            # this is where the subspace magic happens\n",
    "            A = np.concatenate((theta_init, random_init), axis=1) # first column is initializations\n",
    "            p, _ = np.linalg.qr(A)\n",
    "            self.P = Variable(torch.Tensor(p), requires_grad=False)\n",
    "\n",
    "            omega = torch.zeros(self.omega_dim,1)\n",
    "            omega[0] = theta_init[0,0] / p[0,0] # multiply the first column by this to get theta_init values\n",
    "            self.omega = nn.Parameter(omega, requires_grad=True)\n",
    "        \n",
    "    def get_flat_theta(self):\n",
    "        if self.omega_dim is None:\n",
    "            return self.flat_theta\n",
    "        else:\n",
    "            return self.P.mm(self.omega).resize(self.theta_dim) # project from omega space to theta space\n",
    "\n",
    "    def forward(self, X):\n",
    "        flat_theta = self.get_flat_theta()\n",
    "        thetas = {n: flat_theta[self.slices[i]:self.slices[i+1]] for i, n in enumerate(self.names)}\n",
    "        thetas = {k : v.resize(self.param_meta[k][0], self.param_meta[k][1]) for k, v in thetas.items()}\n",
    "        \n",
    "        bs = X.size(0)\n",
    "        h1 = F.relu(X.mm(thetas['W1']) + thetas['b1'].repeat(bs, 1))\n",
    "        h2 = F.relu(h1.mm(thetas['W2']) + thetas['b2'].repeat(bs, 1))\n",
    "        h3 = F.log_softmax(h2.mm(thetas['W3']) + thetas['b3'].repeat(bs, 1))\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train several models\n",
    "Train an MNIST classifier in $\\omega$ spaces of dimensions $[3, 10, 30, 100, 300, 1000]$ as well as in full theta-space. Save losses and test accuracies for each of these training processes. We'll plot them later to compare the convergence properties of the network for various dimensions of $\\omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model summary:\n",
      "\tthis model's omega space has None parameters\n",
      "\tthis model's theta space has 199210 parameters\n",
      "\ttraining...\n",
      "\tstep 131/11718 | accuracy: 91.1558% (9101/9984) | loss: 0.9628\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-49e5973d9024>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubspaceNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0momega_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/code_ideas/subspace-nn/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, global_step, total_steps, test_every)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Sam/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train models with different omega subspace sizes, saving loss and accuracies for each\n",
    "model = None\n",
    "histories = []\n",
    "for i, omega_dim in enumerate(args.omega_dims):\n",
    "    reseed()\n",
    "    model = SubspaceNN(args.batch_size, args.input_dim, args.hidden_dim, args.target_dim, omega_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.99), weight_decay=0)\n",
    "    h = train(dataloader, model, optimizer, global_step, total_steps, args.test_every)\n",
    "    histories.append(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2 = plt.figure(figsize=[8,5])\n",
    "for i, omega_dim in enumerate(args.omega_dims):\n",
    "    omega_dim = model.theta_dim if omega_dim is None else omega_dim\n",
    "    loss_hist = histories[i][0]\n",
    "    xy = np.stack(loss_hist)\n",
    "    plt.plot(xy[:,0], xy[:,1], linewidth=3.0, label='[$\\omega$]={}'.format(omega_dim))\n",
    "\n",
    "title = \"2-layer fc NN loss on MNIST ([$\\Theta$]={})\".format(model.theta_dim)\n",
    "plt.title(title, fontsize=16)\n",
    "plt.xlabel('train step', fontsize=14) ; plt.setp(plt.gca().axes.get_xticklabels(), fontsize=14)\n",
    "plt.ylabel('loss', fontsize=14) ; plt.setp(plt.gca().axes.get_yticklabels(), fontsize=14)\n",
    "plt.ylim([0,2.5])\n",
    "plt.legend()\n",
    "\n",
    "plt.show() # ; f2.savefig('./{}fc-subspace-loss.png'.format(args.fig_dir), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f3 = plt.figure(figsize=[8,5])\n",
    "for i, omega_dim in enumerate(args.omega_dims):\n",
    "    omega_dim = model.theta_dim if omega_dim is None else omega_dim\n",
    "    acc_hist = histories[i][1]\n",
    "    xy = np.stack(acc_hist)\n",
    "    plt.plot(xy[:,0], xy[:,1], linewidth=3.0, label='[$\\omega$]={}'.format(omega_dim))\n",
    "\n",
    "title = \"2-layer fc NN test accuracy on MNIST ([$\\Theta$]={})\".format(model.theta_dim)\n",
    "plt.title(title, fontsize=16)\n",
    "plt.xlabel('train step', fontsize=14) ; plt.setp(plt.gca().axes.get_xticklabels(), fontsize=14)\n",
    "plt.ylabel('accuracy (%)', fontsize=14) ; plt.setp(plt.gca().axes.get_yticklabels(), fontsize=14)\n",
    "\n",
    "results_msg = 'epochs: {}\\nlearning rate : {}\\nbatch size: {}\\nmax accuracy: {:.2f}%'\\\n",
    "    .format(args.epochs, args.lr, args.batch_size, acc_hist[-1][-1])\n",
    "f3.text(0.92, .50, results_msg, ha='left', va='center', fontsize=12)\n",
    "plt.ylim([0,100])\n",
    "plt.legend()\n",
    "\n",
    "plt.show() ; f3.savefig('./{}fc-subspace-acc.png'.format(args.fig_dir), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
