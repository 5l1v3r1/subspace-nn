{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Generative Adversarial Network (PyTorch)\n",
    "Sam Greydanus. 22 April 2017. MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 8\n",
    "H = 100\n",
    "D_img = 28*28\n",
    "D_labels = 10\n",
    "D_ent = 100\n",
    "print_every = 100\n",
    "global_step = 0\n",
    "iters = 50000\n",
    "\n",
    "d_steps = 1\n",
    "g_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modes = ['train', 'val']\n",
    "trans = transforms.Compose([transforms.ToTensor(),]) # transforms.Normalize((0.1307,), (0.3081,))\n",
    "dsets = {k: datasets.MNIST('./data', train=k=='train', download=True, transform=trans) for k in modes}\n",
    "loaders = {k: torch.utils.data.DataLoader(dsets[k], batch_size=batch_size, shuffle=True) for k in modes}\n",
    "\n",
    "dset_sizes = {k: len(dsets[k]) for k in modes}\n",
    "dset_dims = dsets['train'].train\n",
    "dset_classes = [str(x) for x in range(D_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABbCAYAAABj7n4EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1087fbe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.1307, 0.1307, 0.1307])\n",
    "#     std = np.array([0.3081, 0.3081, 0.3081])\n",
    "#     inp = std * inp + mean\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "inputs, classes = next(iter(loaders['train']))\n",
    "out = utils.make_grid(inputs)\n",
    "imshow(out, title=[dset_classes[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build G and D networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
    "#         self.linear_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.linear_3 = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "#         x = F.elu(self.linear_2(x))\n",
    "        return F.sigmoid(self.linear_3(x))\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
    "#         self.linear_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.linear_3 = nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "#         x = F.elu(self.linear_2(x))\n",
    "        return F.sigmoid(self.linear_3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D = Discriminator()\n",
    "D = Discriminator(input_size=D_img, hidden_size=H, output_size=1)\n",
    "G = Generator(input_size=D_ent, hidden_size=H, output_size=D_img)\n",
    "criterion = nn.BCELoss()  # binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train G and D\n",
    "Structure inspired by Dev Nag's [GAN blog post](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: d_loss: 0.606189 and g_loss: 0.758556\n",
      "step 200: d_loss: 0.575474 and g_loss: 0.532755\n",
      "step 300: d_loss: 0.561136 and g_loss: 0.452629\n",
      "step 400: d_loss: 0.557092 and g_loss: 0.421093\n",
      "step 500: d_loss: 0.554835 and g_loss: 0.412101\n",
      "step 600: d_loss: 0.552553 and g_loss: 0.408122\n",
      "step 700: d_loss: 0.550778 and g_loss: 0.406282\n",
      "step 800: d_loss: 0.549855 and g_loss: 0.407003\n",
      "step 900: d_loss: 0.551028 and g_loss: 0.406307\n",
      "step 1000: d_loss: 0.550068 and g_loss: 0.40595\n",
      "step 1100: d_loss: 0.55008 and g_loss: 0.404719\n",
      "step 1200: d_loss: 0.552282 and g_loss: 0.404118\n",
      "step 1300: d_loss: 0.550119 and g_loss: 0.405296\n",
      "step 1400: d_loss: 0.550534 and g_loss: 0.406339\n",
      "step 1500: d_loss: 0.549976 and g_loss: 0.405586\n",
      "step 1600: d_loss: 0.549701 and g_loss: 0.406104\n",
      "step 1700: d_loss: 0.54926 and g_loss: 0.405582\n",
      "step 1800: d_loss: 0.549491 and g_loss: 0.406453\n",
      "step 1900: d_loss: 0.551117 and g_loss: 0.405861\n",
      "step 2000: d_loss: 0.549543 and g_loss: 0.405533\n",
      "step 2100: d_loss: 0.54995 and g_loss: 0.405596\n",
      "step 2200: d_loss: 0.549506 and g_loss: 0.406684\n",
      "step 2300: d_loss: 0.55217 and g_loss: 0.404628\n",
      "step 2400: d_loss: 0.550728 and g_loss: 0.406506\n",
      "step 2500: d_loss: 0.55097 and g_loss: 0.406969\n",
      "step 2600: d_loss: 0.550456 and g_loss: 0.406156\n",
      "step 2700: d_loss: 0.550411 and g_loss: 0.406213\n",
      "step 2800: d_loss: 0.549472 and g_loss: 0.40612\n",
      "step 2900: d_loss: 0.549021 and g_loss: 0.407274\n",
      "step 3000: d_loss: 0.549177 and g_loss: 0.406308\n",
      "step 3100: d_loss: 0.549355 and g_loss: 0.405917\n",
      "step 3200: d_loss: 0.548956 and g_loss: 0.405789\n",
      "step 3300: d_loss: 0.549145 and g_loss: 0.406377\n",
      "step 3400: d_loss: 0.548516 and g_loss: 0.407609\n",
      "step 3500: d_loss: 0.549212 and g_loss: 0.405969\n",
      "step 3600: d_loss: 0.54908 and g_loss: 0.405772\n",
      "step 3700: d_loss: 0.549016 and g_loss: 0.405262\n",
      "step 3800: d_loss: 0.549063 and g_loss: 0.405772\n",
      "step 3900: d_loss: 0.548557 and g_loss: 0.407\n",
      "step 4000: d_loss: 0.549288 and g_loss: 0.406231\n",
      "step 4100: d_loss: 0.548784 and g_loss: 0.406722\n",
      "step 4200: d_loss: 0.548801 and g_loss: 0.406239\n",
      "step 4300: d_loss: 0.548926 and g_loss: 0.406135\n",
      "step 4400: d_loss: 0.548825 and g_loss: 0.406507\n",
      "step 4500: d_loss: 0.548212 and g_loss: 0.407316\n",
      "step 4600: d_loss: 0.548892 and g_loss: 0.406283\n",
      "step 4700: d_loss: 0.549056 and g_loss: 0.406173\n",
      "step 4800: d_loss: 0.548204 and g_loss: 0.406638\n",
      "step 4900: d_loss: 0.548584 and g_loss: 0.40611\n",
      "step 5000: d_loss: 0.549109 and g_loss: 0.406213\n",
      "step 5100: d_loss: 0.548499 and g_loss: 0.407086\n",
      "step 5200: d_loss: 0.548688 and g_loss: 0.406305\n",
      "step 5300: d_loss: 0.549495 and g_loss: 0.405601\n",
      "step 5400: d_loss: 0.549511 and g_loss: 0.405307\n",
      "step 5500: d_loss: 0.54794 and g_loss: 0.407537\n",
      "step 5600: d_loss: 0.548232 and g_loss: 0.407426\n",
      "step 5700: d_loss: 0.548027 and g_loss: 0.407611\n",
      "step 5800: d_loss: 0.548588 and g_loss: 0.406638\n",
      "step 5900: d_loss: 0.549334 and g_loss: 0.405468\n",
      "step 6000: d_loss: 0.54875 and g_loss: 0.406402\n",
      "step 6100: d_loss: 0.548224 and g_loss: 0.40725\n",
      "step 6200: d_loss: 0.548644 and g_loss: 0.4063\n",
      "step 6300: d_loss: 0.547981 and g_loss: 0.407544\n",
      "step 6400: d_loss: 0.549332 and g_loss: 0.405174\n",
      "step 6500: d_loss: 0.548649 and g_loss: 0.405925\n",
      "step 6600: d_loss: 0.548761 and g_loss: 0.406554\n",
      "step 6700: d_loss: 0.547902 and g_loss: 0.407665\n",
      "step 6800: d_loss: 0.548074 and g_loss: 0.407673\n",
      "step 6900: d_loss: 0.547669 and g_loss: 0.407937\n",
      "step 7000: d_loss: 0.548175 and g_loss: 0.407147\n",
      "step 7100: d_loss: 0.548889 and g_loss: 0.406019\n",
      "step 7200: d_loss: 0.548626 and g_loss: 0.406143\n",
      "step 7300: d_loss: 0.548783 and g_loss: 0.406126\n",
      "step 7400: d_loss: 0.548312 and g_loss: 0.406739\n",
      "step 7500: d_loss: 0.548836 and g_loss: 0.406181\n",
      "step 7600: d_loss: 0.54916 and g_loss: 0.40583\n",
      "step 7700: d_loss: 0.548556 and g_loss: 0.406579\n",
      "step 7800: d_loss: 0.548762 and g_loss: 0.406214\n",
      "step 7900: d_loss: 0.548683 and g_loss: 0.406134\n",
      "step 8000: d_loss: 0.54809 and g_loss: 0.407024\n",
      "step 8100: d_loss: 0.548237 and g_loss: 0.40683\n",
      "step 8200: d_loss: 0.548007 and g_loss: 0.407207\n",
      "step 8300: d_loss: 0.54944 and g_loss: 0.405475\n",
      "step 8400: d_loss: 0.548809 and g_loss: 0.405875\n",
      "step 8500: d_loss: 0.550007 and g_loss: 0.404531\n",
      "step 8600: d_loss: 0.547959 and g_loss: 0.407321\n",
      "step 8700: d_loss: 0.550375 and g_loss: 0.404183\n",
      "step 8800: d_loss: 0.548994 and g_loss: 0.405813\n",
      "step 8900: d_loss: 0.546976 and g_loss: 0.408143\n",
      "step 9000: d_loss: 0.547627 and g_loss: 0.407253\n",
      "step 9100: d_loss: 0.547759 and g_loss: 0.407093\n",
      "step 9200: d_loss: 0.547198 and g_loss: 0.408049\n",
      "step 9300: d_loss: 0.548716 and g_loss: 0.406155\n",
      "step 9400: d_loss: 0.54787 and g_loss: 0.407312\n",
      "step 9500: d_loss: 0.548626 and g_loss: 0.406165\n",
      "step 9600: d_loss: 0.548035 and g_loss: 0.406887\n",
      "step 9700: d_loss: 0.549216 and g_loss: 0.405521\n",
      "step 9800: d_loss: 0.548663 and g_loss: 0.406222\n",
      "step 9900: d_loss: 0.548451 and g_loss: 0.406381\n",
      "step 10000: d_loss: 0.548897 and g_loss: 0.405893\n",
      "step 10100: d_loss: 0.550379 and g_loss: 0.404431\n",
      "step 10200: d_loss: 0.550036 and g_loss: 0.404794\n",
      "step 10300: d_loss: 0.548745 and g_loss: 0.40608\n",
      "step 10400: d_loss: 0.549092 and g_loss: 0.405734\n",
      "step 10500: d_loss: 0.54876 and g_loss: 0.406098\n",
      "step 10600: d_loss: 0.549738 and g_loss: 0.405059\n",
      "step 10700: d_loss: 0.549302 and g_loss: 0.405477\n",
      "step 10800: d_loss: 0.548996 and g_loss: 0.405892\n",
      "step 10900: d_loss: 0.549123 and g_loss: 0.405652\n",
      "step 11000: d_loss: 0.549672 and g_loss: 0.405264\n",
      "step 11100: d_loss: 0.549028 and g_loss: 0.405865\n",
      "step 11200: d_loss: 0.549863 and g_loss: 0.405043\n",
      "step 11300: d_loss: 0.549295 and g_loss: 0.405635\n",
      "step 11400: d_loss: 0.549852 and g_loss: 0.404983\n",
      "step 11500: d_loss: 0.549246 and g_loss: 0.405659\n",
      "step 11600: d_loss: 0.549227 and g_loss: 0.405653\n",
      "step 11700: d_loss: 0.549742 and g_loss: 0.405128\n",
      "step 11800: d_loss: 0.548999 and g_loss: 0.405845\n",
      "step 11900: d_loss: 0.548117 and g_loss: 0.4066\n",
      "step 12000: d_loss: 0.550104 and g_loss: 0.404837\n",
      "step 12100: d_loss: 0.551061 and g_loss: 0.403972\n",
      "step 12200: d_loss: 0.549121 and g_loss: 0.405772\n",
      "step 12300: d_loss: 0.549406 and g_loss: 0.405301\n",
      "step 12400: d_loss: 0.550112 and g_loss: 0.404848\n",
      "step 12500: d_loss: 0.550301 and g_loss: 0.404676\n",
      "step 12600: d_loss: 0.549855 and g_loss: 0.405027\n",
      "step 12700: d_loss: 0.549534 and g_loss: 0.405298\n",
      "step 12800: d_loss: 0.549389 and g_loss: 0.40541\n",
      "step 12900: d_loss: 0.549265 and g_loss: 0.405506\n",
      "step 13000: d_loss: 0.549402 and g_loss: 0.405399\n",
      "step 13100: d_loss: 0.549345 and g_loss: 0.405439\n",
      "step 13200: d_loss: 0.54932 and g_loss: 0.405457\n",
      "step 13300: d_loss: 0.549281 and g_loss: 0.40548\n",
      "step 13400: d_loss: 0.548748 and g_loss: 0.406003\n",
      "step 13500: d_loss: 0.54998 and g_loss: 0.404971\n",
      "step 13600: d_loss: 0.551531 and g_loss: 0.403506\n",
      "step 13700: d_loss: 0.548925 and g_loss: 0.406109\n",
      "step 13800: d_loss: 0.549147 and g_loss: 0.405761\n",
      "step 13900: d_loss: 0.548736 and g_loss: 0.406092\n",
      "step 14000: d_loss: 0.549905 and g_loss: 0.404945\n",
      "step 14100: d_loss: 0.549156 and g_loss: 0.405812\n",
      "step 14200: d_loss: 0.548986 and g_loss: 0.405836\n",
      "step 14300: d_loss: 0.549526 and g_loss: 0.405281\n",
      "step 14400: d_loss: 0.550129 and g_loss: 0.405607\n",
      "step 14500: d_loss: 0.549797 and g_loss: 0.40536\n",
      "step 14600: d_loss: 0.550573 and g_loss: 0.405392\n",
      "step 14700: d_loss: 0.550894 and g_loss: 0.404587\n",
      "step 14800: d_loss: 0.550695 and g_loss: 0.404441\n",
      "step 14900: d_loss: 0.549659 and g_loss: 0.405246\n",
      "step 15000: d_loss: 0.550051 and g_loss: 0.404777\n",
      "step 15100: d_loss: 0.549547 and g_loss: 0.405233\n",
      "step 15200: d_loss: 0.549036 and g_loss: 0.405714\n",
      "step 15300: d_loss: 0.549206 and g_loss: 0.405558\n",
      "step 15400: d_loss: 0.549271 and g_loss: 0.405498\n",
      "step 15500: d_loss: 0.548413 and g_loss: 0.406238\n",
      "step 15600: d_loss: 0.548831 and g_loss: 0.405877\n",
      "step 15700: d_loss: 0.555639 and g_loss: 0.397522\n",
      "step 15800: d_loss: 0.547554 and g_loss: 0.407061\n",
      "step 15900: d_loss: 0.548656 and g_loss: 0.406057\n",
      "step 16000: d_loss: 0.549068 and g_loss: 0.405681\n",
      "step 16100: d_loss: 0.549218 and g_loss: 0.405544\n",
      "step 16200: d_loss: 0.549273 and g_loss: 0.405493\n",
      "step 16300: d_loss: 0.549293 and g_loss: 0.405475\n",
      "step 16400: d_loss: 0.549301 and g_loss: 0.405468\n",
      "step 16500: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 16600: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 16700: d_loss: 0.549302 and g_loss: 0.405465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16800: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 16900: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17000: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17100: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17200: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17300: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17400: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17500: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17600: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17700: d_loss: 0.549304 and g_loss: 0.405465\n",
      "step 17800: d_loss: 0.549416 and g_loss: 0.405505\n",
      "step 17900: d_loss: 0.549345 and g_loss: 0.405482\n",
      "step 18000: d_loss: 0.54932 and g_loss: 0.405471\n",
      "step 18100: d_loss: 0.549311 and g_loss: 0.405468\n",
      "step 18200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 18300: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 18400: d_loss: 0.55034 and g_loss: 0.404511\n",
      "step 18500: d_loss: 0.549242 and g_loss: 0.405527\n",
      "step 18600: d_loss: 0.549282 and g_loss: 0.405488\n",
      "step 18700: d_loss: 0.549297 and g_loss: 0.405474\n",
      "step 18800: d_loss: 0.549302 and g_loss: 0.405468\n",
      "step 18900: d_loss: 0.549302 and g_loss: 0.405468\n",
      "step 19000: d_loss: 0.549302 and g_loss: 0.405468\n",
      "step 19100: d_loss: 0.549302 and g_loss: 0.405468\n",
      "step 19200: d_loss: 0.549302 and g_loss: 0.405468\n",
      "step 19300: d_loss: 0.549302 and g_loss: 0.405468\n",
      "step 19400: d_loss: 0.549302 and g_loss: 0.405467\n",
      "step 19500: d_loss: 0.549302 and g_loss: 0.405467\n",
      "step 19600: d_loss: 0.549302 and g_loss: 0.405467\n",
      "step 19700: d_loss: 0.549302 and g_loss: 0.405466\n",
      "step 19800: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 19900: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 20000: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 20100: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 20200: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 20300: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 20400: d_loss: 0.549302 and g_loss: 0.405465\n",
      "step 20500: d_loss: 0.549315 and g_loss: 0.40547\n",
      "step 20600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 20700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 20800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 20900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21100: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 21800: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 21900: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 22000: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 22100: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 22200: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 22300: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 22400: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 22500: d_loss: 0.549343 and g_loss: 0.405482\n",
      "step 22600: d_loss: 0.549319 and g_loss: 0.405471\n",
      "step 22700: d_loss: 0.549311 and g_loss: 0.405468\n",
      "step 22800: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 22900: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23000: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23100: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23200: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23300: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23400: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23500: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23600: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23700: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23800: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 23900: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 24000: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24100: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24200: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24300: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24400: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24500: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24600: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24700: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24800: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 24900: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25000: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25100: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25200: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25300: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25400: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25500: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25600: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25700: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25800: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 25900: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 26000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 26900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 27900: d_loss: 0.551046 and g_loss: 0.406432\n",
      "step 28000: d_loss: 0.549972 and g_loss: 0.405777\n",
      "step 28100: d_loss: 0.54955 and g_loss: 0.40558\n",
      "step 28200: d_loss: 0.549395 and g_loss: 0.405507\n",
      "step 28300: d_loss: 0.549338 and g_loss: 0.405481\n",
      "step 28400: d_loss: 0.549317 and g_loss: 0.405472\n",
      "step 28500: d_loss: 0.54931 and g_loss: 0.405468\n",
      "step 28600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 28700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 28800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 28900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29100: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 29900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30100: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 30900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31100: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 31900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32100: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 32900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33100: d_loss: 0.549308 and g_loss: 0.405468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 33200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 33900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34100: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 34900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35100: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35200: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35300: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35400: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35500: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35600: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35700: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35800: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 35900: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 36000: d_loss: 0.549308 and g_loss: 0.405468\n",
      "step 36100: d_loss: 0.549308 and g_loss: 0.405467\n",
      "step 36200: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 36300: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 36400: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 36500: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 36600: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 36700: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 36800: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 36900: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37000: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37100: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37200: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37300: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37400: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37500: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37600: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 37700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 37800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 37900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 38900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 39900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 40900: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 41000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41100: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 41200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 41900: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 42000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42500: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 42600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 42900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43200: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 43300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 43900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44000: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 44100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44400: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 44500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 44900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 45900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46400: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 46500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 46900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47100: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 47200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 47900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48500: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 48800: d_loss: 0.549308 and g_loss: 0.405466\n",
      "step 48900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49000: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49100: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49200: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49300: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49400: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49500: d_loss: 0.549308 and g_loss: 0.405465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 49600: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49700: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49800: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 49900: d_loss: 0.549308 and g_loss: 0.405465\n",
      "step 50000: d_loss: 0.549308 and g_loss: 0.405465\n"
     ]
    }
   ],
   "source": [
    "ones_label = Variable(torch.ones(batch_size,1))\n",
    "zeros_label = Variable(torch.zeros(batch_size,1))\n",
    "\n",
    "D_loss_avg = G_loss_avg = None ; interp = 0.99\n",
    "for global_step in range(global_step+1,global_step+1+iters):\n",
    "    # Sample data\n",
    "    z = Variable(torch.randn(batch_size, D_ent))\n",
    "    X, _ = next(iter(loaders['train']))\n",
    "    X = Variable(X).resize(batch_size,D_img)\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X) # .resize(batch_size,1,28,28)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    D_loss_real = criterion(D_real, ones_label)\n",
    "    D_loss_fake = criterion(D_fake, zeros_label)\n",
    "    D_loss = .5*(D_loss_real + D_loss_fake)\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "    D.zero_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    z = Variable(torch.randn(batch_size, D_ent))\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = criterion(D_fake, ones_label)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "    G.zero_grad()\n",
    "    \n",
    "    D_loss_avg = D_loss if D_loss_avg is None else interp*D_loss_avg + (1-interp)*D_loss\n",
    "    G_loss_avg = G_loss if G_loss_avg is None else interp*G_loss_avg + (1-interp)*G_loss\n",
    "    \n",
    "    if global_step % print_every == 0:\n",
    "        print(\"step %s: d_loss: %s and g_loss: %s\"% (global_step,\n",
    "                                                     D_loss_avg.data.numpy()[0],\n",
    "                                                     G_loss_avg.data.numpy()[0]))\n",
    "#         print(\"\\tD_loss_real: %s and D_loss_fake: %s\" % (D_loss_real.data.numpy()[0],\n",
    "#                                                          D_loss_fake.data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABPCAYAAAD7qT6JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5fdd358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABPCAYAAAD7qT6JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng64oihIk/D/kKJeE/AZ8FAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5fd3978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "z = Variable(torch.randn(batch_size, D_ent))\n",
    "fake = G(z).resize(batch_size,1,28,28).data\n",
    "imshow(utils.make_grid(fake))\n",
    "\n",
    "plt.figure(1)\n",
    "inputs, classes = next(iter(loaders['train']))\n",
    "imshow(utils.make_grid(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def entropy():\n",
    "#     return torch.rand(batch_size, D_ent)\n",
    "\n",
    "# avg_d_loss = avg_g_loss = None ; interp = 0.99\n",
    "# for global_step in range(global_step+1,global_step+1+iters):\n",
    "#     for d_index in range(d_steps):\n",
    "#         # 1. Train D on real+fake\n",
    "#         D.zero_grad()\n",
    "\n",
    "#         #  1A: Train D on real\n",
    "#         d_real_data, real_classes = next(iter(loaders['train']))\n",
    "#         x = Variable(d_real_data)\n",
    "#         d_real_decision = D(x)\n",
    "#         d_real_error = criterion(d_real_decision, Variable(torch.ones(batch_size, 1)))  # ones = true\n",
    "#         d_real_error.backward() # compute/store gradients, but don't change params\n",
    "\n",
    "#         #  1B: Train D on fake\n",
    "#         x = Variable(entropy())\n",
    "#         d_fake_data = G(x).detach()  # detach to avoid training G on these labels\n",
    "#         d_fake_decision = D(d_fake_data.resize(batch_size,1,28,28))\n",
    "#         d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(batch_size, 1)))  # zeros = fake\n",
    "#         d_fake_error.backward()\n",
    "#         d_optimizer.step()     # Only optimizes D's parameters; changes based on stored gradients from backward()\n",
    "        \n",
    "#         d_loss = .5*(d_real_error.data + d_fake_error.data)\n",
    "\n",
    "#     for g_index in range(3):\n",
    "#         # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "#         G.zero_grad()\n",
    "        \n",
    "#         x = Variable(entropy())\n",
    "#         g_fake_data = G(x).resize(batch_size,1,28,28)\n",
    "#         dg_fake_decision = D(g_fake_data)\n",
    "#         g_error = criterion(dg_fake_decision, Variable(torch.ones(batch_size, 1)))  # pretend it's all genuine\n",
    "\n",
    "#         g_error.backward()\n",
    "#         g_optimizer.step()  # Only optimizes G's parameters\n",
    "        \n",
    "#         g_loss = g_error.data\n",
    "        \n",
    "#     avg_d_loss = d_loss if avg_d_loss is None else interp*avg_d_loss + (1-interp)*d_loss\n",
    "#     avg_g_loss = g_loss if avg_g_loss is None else interp*avg_g_loss + (1-interp)*g_loss\n",
    "\n",
    "#     if global_step % print_every == 0:\n",
    "#         print(\"step %s: d_loss: %s and g_loss: %s\"% (global_step, avg_d_loss.numpy()[0], avg_g_loss.numpy()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
